{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "### Sergio Orellana - 221122\n",
    "\n",
    "### Andre Marroquin - 22266\n",
    "\n",
    "### Rodrigo Mansilla - 22611\n",
    "\n",
    "# Link del repositorio\n",
    "\n",
    "https://github.com/mar22266/LABORATORIOS-IA.git\n",
    "\n",
    "# Link del video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Humano vs IA\n",
      "2. IA vs IA\n",
      "Opción no válida.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "# Parámetros del juego y del algoritmo Q-learning\n",
    "ROW_COUNT = 6\n",
    "COLUMN_COUNT = 7\n",
    "EMPTY = 0\n",
    "\n",
    "# Parámetros del Q-learning\n",
    "ALPHA = 0.001       # Tasa de aprendizaje para la red (usado en el optimizador)\n",
    "GAMMA = 0.95        # Factor de descuento\n",
    "EPSILON = 1.0       # Tasa de exploración inicial\n",
    "EPSILON_MIN = 0.1   # Valor mínimo de epsilon\n",
    "EPSILON_DECAY = 0.995  # Factor de decaimiento de epsilon por episodio\n",
    "\n",
    "# Parámetros del modelo\n",
    "INPUT_SIZE = ROW_COUNT * COLUMN_COUNT * 3  # 126, por la representación one-hot\n",
    "OUTPUT_SIZE = COLUMN_COUNT  # 7 acciones (columnas)\n",
    "\n",
    "# ---------------- Modelo de Red Neuronal ---------------- #\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, input_dim=INPUT_SIZE, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(OUTPUT_SIZE, activation='linear'))  # Q-values para cada acción\n",
    "    model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=ALPHA))\n",
    "    return model\n",
    "\n",
    "# Inicializamos el modelo globalmente\n",
    "q_model = build_model()\n",
    "\n",
    "# ---------------- Funciones del juego ---------------- #\n",
    "def create_board():\n",
    "    board = [[EMPTY for _ in range(COLUMN_COUNT)] for _ in range(ROW_COUNT)]\n",
    "    return board\n",
    "\n",
    "def drop_piece(board, row, col, piece):\n",
    "    board[row][col] = piece\n",
    "\n",
    "def is_valid_location(board, col):\n",
    "    return board[0][col] == EMPTY\n",
    "\n",
    "def get_next_open_row(board, col):\n",
    "    for r in range(ROW_COUNT - 1, -1, -1):\n",
    "        if board[r][col] == EMPTY:\n",
    "            return r\n",
    "    return None\n",
    "\n",
    "def winning_move(board, piece):\n",
    "    # Horizontal\n",
    "    for r in range(ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r][c+1] == piece and \n",
    "                board[r][c+2] == piece and board[r][c+3] == piece):\n",
    "                return True\n",
    "    # Vertical\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c] == piece and \n",
    "                board[r+2][c] == piece and board[r+3][c] == piece):\n",
    "                return True\n",
    "    # Diagonal positiva\n",
    "    for r in range(3, ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r-1][c+1] == piece and \n",
    "                board[r-2][c+2] == piece and board[r-3][c+3] == piece):\n",
    "                return True\n",
    "    # Diagonal negativa\n",
    "    for r in range(ROW_COUNT - 3):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c+1] == piece and \n",
    "                board[r+2][c+2] == piece and board[r+3][c+3] == piece):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_winning_positions(board, piece):\n",
    "    for r in range(ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r][c+1] == piece and \n",
    "                board[r][c+2] == piece and board[r][c+3] == piece):\n",
    "                return [(r, c + i) for i in range(4)]\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c] == piece and \n",
    "                board[r+2][c] == piece and board[r+3][c] == piece):\n",
    "                return [(r + i, c) for i in range(4)]\n",
    "    for r in range(3, ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r-1][c+1] == piece and \n",
    "                board[r-2][c+2] == piece and board[r-3][c+3] == piece):\n",
    "                return [(r - i, c + i) for i in range(4)]\n",
    "    for r in range(ROW_COUNT - 3):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c+1] == piece and \n",
    "                board[r+2][c+2] == piece and board[r+3][c+3] == piece):\n",
    "                return [(r + i, c + i) for i in range(4)]\n",
    "    return []\n",
    "\n",
    "def get_valid_locations(board):\n",
    "    valid_locations = []\n",
    "    for col in range(COLUMN_COUNT):\n",
    "        if is_valid_location(board, col):\n",
    "            valid_locations.append(col)\n",
    "    return valid_locations\n",
    "\n",
    "def is_terminal_node(board):\n",
    "    return winning_move(board, 1) or winning_move(board, 2) or len(get_valid_locations(board)) == 0\n",
    "\n",
    "# Representación del estado: one-hot\n",
    "def get_state_one_hot(board):\n",
    "    mapping = {0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]}\n",
    "    state = []\n",
    "    for row in board:\n",
    "        for cell in row:\n",
    "            state.extend(mapping[cell])\n",
    "    return np.array(state)\n",
    "\n",
    "def print_board(board, winning_positions=[]):\n",
    "    for r in range(ROW_COUNT):\n",
    "        row_str = \"\"\n",
    "        for c in range(COLUMN_COUNT):\n",
    "            cell = board[r][c]\n",
    "            if (r, c) in winning_positions:\n",
    "                row_str += \"\\033[91m\" + str(cell) + \"\\033[0m\" + \" \"\n",
    "            else:\n",
    "                row_str += str(cell) + \" \"\n",
    "        print(row_str)\n",
    "    print(\"\")  # Salto de línea\n",
    "\n",
    "# ---------------- Agente TD Learning con Modelo ---------------- #\n",
    "\n",
    "def choose_action(state, board, epsilon):\n",
    "    \"\"\"\n",
    "    Selecciona una acción usando política ε-greedy.\n",
    "    Si random < epsilon, elige acción aleatoria de entre las válidas.\n",
    "    De lo contrario, elige la acción con mayor Q-value entre las válidas.\n",
    "    \"\"\"\n",
    "    valid_actions = get_valid_locations(board)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(valid_actions)\n",
    "    # Predecir Q-values con el modelo\n",
    "    state_input = state.reshape(1, INPUT_SIZE)\n",
    "    q_values = q_model.predict(state_input, verbose=0)[0]\n",
    "    # Filtrar Q-values solo para acciones válidas\n",
    "    q_valid = {action: q_values[action] for action in valid_actions}\n",
    "    return max(q_valid, key=q_valid.get)\n",
    "\n",
    "def update_Q(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    Actualiza el modelo (red neuronal) usando un paso de Q-learning.\n",
    "    Calcula el target para la acción tomada y realiza una actualización.\n",
    "    \"\"\"\n",
    "    state_input = state.reshape(1, INPUT_SIZE)\n",
    "    next_state_input = next_state.reshape(1, INPUT_SIZE)\n",
    "    # Predicción actual de Q(s, :)\n",
    "    q_values = q_model.predict(state_input, verbose=0)\n",
    "    # Predicción de Q(s', :)\n",
    "    q_next = q_model.predict(next_state_input, verbose=0)\n",
    "    target = q_values.copy()\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + GAMMA * np.max(q_next)\n",
    "    # Entrenamiento de la red para ajustar el Q-value de la acción tomada\n",
    "    q_model.fit(state_input, target, epochs=1, verbose=0)\n",
    "\n",
    "# ---------------- Flujo de Juego ---------------- #\n",
    "\n",
    "# Variable global para acceder al tablero actual en choose_action\n",
    "current_board = None\n",
    "\n",
    "def iniciar_juego(mode=\"human_vs_ai\", depth=4, episodes=1):\n",
    "    global current_board, EPSILON\n",
    "    board = create_board()\n",
    "    current_board = board\n",
    "    game_over = False\n",
    "    turn = 0\n",
    "    print_board(board)\n",
    "    while not game_over:\n",
    "        if mode == \"human_vs_ai\":\n",
    "            if turn == 0:\n",
    "                # Turno del humano\n",
    "                valid_cols = get_valid_locations(board)\n",
    "                col = -1\n",
    "                while col not in valid_cols:\n",
    "                    try:\n",
    "                        col = int(input(f\"Col (0-{COLUMN_COUNT-1}): \"))\n",
    "                        if col not in valid_cols:\n",
    "                            print(\"No válido, intente otra vez.\")\n",
    "                    except ValueError:\n",
    "                        print(\"Entrada inválida.\")\n",
    "                row = get_next_open_row(board, col)\n",
    "                drop_piece(board, row, col, 1)\n",
    "                if winning_move(board, 1):\n",
    "                    winning_pos = get_winning_positions(board, 1)\n",
    "                    print_board(board, winning_pos)\n",
    "                    print(\"¡Ganaste!\")\n",
    "                    game_over = True\n",
    "                else:\n",
    "                    print_board(board)\n",
    "            else:\n",
    "                # Turno de la IA con TD Learning\n",
    "                print(\"IA pensando (TD Learning)...\")\n",
    "                state = get_state_one_hot(board)\n",
    "                col = choose_action(state, board, EPSILON)\n",
    "                if col not in get_valid_locations(board):\n",
    "                    col = random.choice(get_valid_locations(board))\n",
    "                row = get_next_open_row(board, col)\n",
    "                drop_piece(board, row, col, 2)\n",
    "                \n",
    "                if winning_move(board, 2):\n",
    "                    reward = 100\n",
    "                    game_over = True\n",
    "                elif is_terminal_node(board):\n",
    "                    reward = 0\n",
    "                    game_over = True\n",
    "                else:\n",
    "                    reward = -1\n",
    "                \n",
    "                next_state = get_state_one_hot(board)\n",
    "                update_Q(state, col, reward, next_state, game_over)\n",
    "                \n",
    "                if winning_move(board, 2):\n",
    "                    winning_pos = get_winning_positions(board, 2)\n",
    "                    print_board(board, winning_pos)\n",
    "                    print(\"IA gana.\")\n",
    "                else:\n",
    "                    print_board(board)\n",
    "                \n",
    "                # Decaimiento de epsilon\n",
    "                if EPSILON > EPSILON_MIN:\n",
    "                    EPSILON *= EPSILON_DECAY\n",
    "                \n",
    "            turn = (turn + 1) % 2\n",
    "            current_board = board\n",
    "        elif mode == \"ai_vs_ai\":\n",
    "            # Conservamos el modo IA vs IA con minimax (no modificado aquí)\n",
    "            if turn == 0:\n",
    "                print(\"IA sin poda...\")\n",
    "                use_pruning = False\n",
    "                col, score = minimax(board, depth, -math.inf, math.inf, True, 1, use_pruning)\n",
    "                piece = 1\n",
    "            else:\n",
    "                print(\"IA con poda...\")\n",
    "                use_pruning = True\n",
    "                col, score = minimax(board, depth, -math.inf, math.inf, True, 2, use_pruning)\n",
    "                piece = 2\n",
    "            if col is None:\n",
    "                print(\"Empate!\")\n",
    "                game_over = True\n",
    "            else:\n",
    "                row = get_next_open_row(board, col)\n",
    "                drop_piece(board, row, col, piece)\n",
    "                if winning_move(board, piece):\n",
    "                    winning_pos = get_winning_positions(board, piece)\n",
    "                    print_board(board, winning_pos)\n",
    "                    if turn == 0:\n",
    "                        print(\"IA sin poda gana!\")\n",
    "                    else:\n",
    "                        print(\"IA con poda gana!\")\n",
    "                    game_over = True\n",
    "                else:\n",
    "                    print_board(board)\n",
    "            turn = (turn + 1) % 2\n",
    "        else:\n",
    "            print(\"Modo no válido.\")\n",
    "            break\n",
    "\n",
    "def ejecutar():\n",
    "    print(\"1. Humano vs IA\")\n",
    "    print(\"2. IA vs IA\")\n",
    "    mode_input = input(\"Elija 1 o 2: \")\n",
    "    if mode_input == \"1\":\n",
    "        print(\"Modo: Humano vs IA\")\n",
    "        iniciar_juego(mode=\"human_vs_ai\")\n",
    "    elif mode_input == \"2\":\n",
    "        print(\"Modo: IA vs IA\")\n",
    "        iniciar_juego(mode=\"ai_vs_ai\")\n",
    "    else:\n",
    "        print(\"Opción no válida.\")\n",
    "\n",
    "ejecutar()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
