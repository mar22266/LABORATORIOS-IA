{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "### Sergio Orellana - 221122\n",
    "\n",
    "### Andre Marroquin - 22266\n",
    "\n",
    "### Rodrigo Mansilla - 22611\n",
    "\n",
    "# Link del repositorio\n",
    "\n",
    "https://github.com/mar22266/LABORATORIOS-IA.git\n",
    "\n",
    "# Link del video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es el aprendizaje por diferencias temporales (Temporal Difference Learning) y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo.\n",
    "\n",
    "El aprendizaje por diferencias temporales TD es una técnica de aprendizaje por refuerzo que permite a un agente aprender a predecir valores futuros basándose en experiencias pasadas, sin necesidad de esperar una señal de recompensa final. A diferencia del aprendizaje supervisado, que requiere un conjunto de datos etiquetados para entrenar un modelo, el aprendizaje TD ajusta sus estimaciones de valor en función de la diferencia entre predicciones sucesivas a lo largo del tiempo.\n",
    "\n",
    "El \"error de diferencia temporal\" es la discrepancia entre la recompensa observada más el valor estimado del siguiente estado y el valor estimado del estado actual. Este error guía la actualización de las estimaciones de valor del agente, permitiéndole mejorar sus predicciones y decisiones futuras.\n",
    "\n",
    "\n",
    "2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes? Dé un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación.\n",
    "\n",
    "En los juegos simultáneos, los jugadores deben elegir sus estrategias al mismo tiempo, sin información sobre las elecciones de los demás. Para tomar decisiones, cada jugador considera las posibles acciones de los oponentes y evalúa las mejores respuestas a esas acciones, utilizando conceptos como el equilibrio de Nash para identificar estrategias óptimas.\n",
    "\n",
    "Un ejemplo del mundo real es la competencia entre empresas que lanzan productos similares al mercado. Sin conocer las decisiones exactas de sus competidores, cada empresa debe decidir aspectos como el precio, la calidad y las características del producto. Las estrategias pueden incluir la diferenciación del producto, la competencia en precios o la innovación para captar una mayor cuota de mercado.\n",
    "\n",
    "\n",
    "3. ¿Qué distingue los juegos de suma cero de los juegos de no suma cero y cómo afecta esta diferencia al proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas.\n",
    "\n",
    "En los juegos de suma cero, la ganancia de un jugador es exactamente igual a la pérdida de otro; es decir, el total de ganancias y pérdidas suma cero. En contraste, en los juegos de no suma cero, es posible que todos los jugadores ganen o pierdan simultáneamente, permitiendo resultados de beneficio mutuo o perjuicio compartido.\n",
    "\n",
    "Esta diferencia influye en la toma de decisiones: en juegos de suma cero, los jugadores suelen adoptar estrategias competitivas enfocadas en maximizar su propia ganancia a expensas del oponente. En juegos de no suma cero, las estrategias pueden ser más cooperativas, buscando soluciones que beneficien a todas las partes involucradas.\n",
    "\n",
    "Un ejemplo de juego de no suma cero es el dilema del prisionero, donde dos sospechosos deciden independientemente si confesar o no. La cooperación no confesar puede resultar mejor para ambos, mientras que la traición mutua ambos confiesan lleva a resultados peores. Este escenario destaca la tensión entre la cooperación y la competencia en la toma de decisiones estratégicas.\n",
    "\n",
    "\n",
    "4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explique cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida.\n",
    "\n",
    "El equilibrio de Nash en juegos simultáneos es una situación en la que cada jugador adopta la mejor estrategia posible, considerando las estrategias elegidas por los demás jugadores. En este equilibrio, ningún jugador puede mejorar su resultado cambiando unilateralmente su estrategia, lo que lo convierte en una solución estable.\n",
    "\n",
    "Este concepto es fundamental en la teoría de juegos, ya que permite predecir resultados en situaciones donde los participantes toman decisiones interdependientes. En un equilibrio de Nash, cada jugador reconoce que cualquier desviación unilateral no le proporcionará un beneficio adicional, lo que fomenta la estabilidad de las estrategias adoptadas.\n",
    "\n",
    "\n",
    "5. Discuta la aplicación del aprendizaje por diferencias temporales en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos. ¿Cómo maneja el aprendizaje por diferencias temporales el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la práctica?\n",
    "\n",
    "El aprendizaje por diferencias temporales (TD) se aplica en entornos dinámicos para modelar y optimizar procesos de toma de decisiones, permitiendo a un agente aprender políticas óptimas a través de la interacción continua con el entorno. Al actualizar las estimaciones de valor basándose en experiencias recientes, el aprendizaje TD es eficaz en situaciones donde el modelo del entorno es desconocido o cambia con el tiempo.\n",
    "\n",
    "Para equilibrar la exploración, descubrir nuevas estrategias y la explotación utilizar estrategias conocidas que ofrecen recompensas, se emplean políticas como e-greedy, donde el agente elige acciones aleatorias con una pequeña probabilidad e y, en otros casos, selecciona la acción que maximiza la recompensa esperada.\n",
    "\n",
    "Los desafíos en la implementación práctica del aprendizaje TD incluyen la selección adecuada de la tasa de aprendizaje y el manejo de la maldición de la dimensionalidad, que se refiere al aumento exponencial de la complejidad a medida que crece el espacio de estados y acciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones del Juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Funciones del juego ---------------- #\n",
    "def create_board():\n",
    "    return [[EMPTY for _ in range(COLUMN_COUNT)] for _ in range(ROW_COUNT)]\n",
    "\n",
    "def drop_piece(board, row, col, piece):\n",
    "    board[row][col] = piece\n",
    "\n",
    "def is_valid_location(board, col):\n",
    "    return board[0][col] == EMPTY\n",
    "\n",
    "def get_next_open_row(board, col):\n",
    "    for r in range(ROW_COUNT - 1, -1, -1):\n",
    "        if board[r][col] == EMPTY:\n",
    "            return r\n",
    "    return None\n",
    "\n",
    "def winning_move(board, piece):\n",
    "    # Horizontal\n",
    "    for r in range(ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r][c+1] == piece and \n",
    "                board[r][c+2] == piece and board[r][c+3] == piece):\n",
    "                return True\n",
    "    # Vertical\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c] == piece and \n",
    "                board[r+2][c] == piece and board[r+3][c] == piece):\n",
    "                return True\n",
    "    # Diagonal positiva\n",
    "    for r in range(3, ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r-1][c+1] == piece and \n",
    "                board[r-2][c+2] == piece and board[r-3][c+3] == piece):\n",
    "                return True\n",
    "    # Diagonal negativa\n",
    "    for r in range(ROW_COUNT - 3):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c+1] == piece and \n",
    "                board[r+2][c+2] == piece and board[r+3][c+3] == piece):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_winning_positions(board, piece):\n",
    "    for r in range(ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r][c+1] == piece and \n",
    "                board[r][c+2] == piece and board[r][c+3] == piece):\n",
    "                return [(r, c + i) for i in range(4)]\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c] == piece and \n",
    "                board[r+2][c] == piece and board[r+3][c] == piece):\n",
    "                return [(r + i, c) for i in range(4)]\n",
    "    for r in range(3, ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r-1][c+1] == piece and \n",
    "                board[r-2][c+2] == piece and board[r-3][c+3] == piece):\n",
    "                return [(r - i, c + i) for i in range(4)]\n",
    "    for r in range(ROW_COUNT - 3):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c+1] == piece and \n",
    "                board[r+2][c+2] == piece and board[r+3][c+3] == piece):\n",
    "                return [(r + i, c + i) for i in range(4)]\n",
    "    return []\n",
    "\n",
    "def get_valid_locations(board):\n",
    "    valid = []\n",
    "    for col in range(COLUMN_COUNT):\n",
    "        if is_valid_location(board, col):\n",
    "            valid.append(col)\n",
    "    return valid\n",
    "\n",
    "def is_terminal_node(board):\n",
    "    return winning_move(board, 1) or winning_move(board, 2) or len(get_valid_locations(board)) == 0\n",
    "\n",
    "# Representación del estado: one-hot\n",
    "def get_state_one_hot(board):\n",
    "    mapping = {0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]}\n",
    "    state = []\n",
    "    for row in board:\n",
    "        for cell in row:\n",
    "            state.extend(mapping[cell])\n",
    "    return np.array(state)\n",
    "\n",
    "def print_board(board, winning_positions=[]):\n",
    "    for r in range(ROW_COUNT):\n",
    "        row_str = \"\"\n",
    "        for c in range(COLUMN_COUNT):\n",
    "            cell = board[r][c]\n",
    "            if (r, c) in winning_positions:\n",
    "                row_str += \"\\033[91m\" + str(cell) + \"\\033[0m\" + \" \"\n",
    "            else:\n",
    "                row_str += str(cell) + \" \"\n",
    "        print(row_str)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo minimax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Implementación del Algoritmo Minimax ---------------- #\n",
    "def minimax(board, depth, alpha, beta, maximizingPlayer, piece, use_pruning):\n",
    "    valid_locations = get_valid_locations(board)\n",
    "    terminal = is_terminal_node(board)\n",
    "    opponent_piece = 1 if piece == 2 else 2\n",
    "    if depth == 0 or terminal:\n",
    "        if terminal:\n",
    "            if winning_move(board, piece):\n",
    "                return (None, math.inf)\n",
    "            elif winning_move(board, opponent_piece):\n",
    "                return (None, -math.inf)\n",
    "            else:\n",
    "                return (None, 0)\n",
    "        else:\n",
    "            return (None, score_position(board, piece))\n",
    "    if maximizingPlayer:\n",
    "        value = -math.inf\n",
    "        best_col = random.choice(valid_locations)\n",
    "        for col in valid_locations:\n",
    "            row = get_next_open_row(board, col)\n",
    "            b_copy = copy.deepcopy(board)\n",
    "            drop_piece(b_copy, row, col, piece)\n",
    "            new_score = minimax(b_copy, depth - 1, alpha, beta, False, opponent_piece, use_pruning)[1]\n",
    "            if new_score > value:\n",
    "                value = new_score\n",
    "                best_col = col\n",
    "            if use_pruning:\n",
    "                alpha = max(alpha, value)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "        return best_col, value\n",
    "    else:\n",
    "        value = math.inf\n",
    "        best_col = random.choice(valid_locations)\n",
    "        for col in valid_locations:\n",
    "            row = get_next_open_row(board, col)\n",
    "            b_copy = copy.deepcopy(board)\n",
    "            drop_piece(b_copy, row, col, piece)\n",
    "            new_score = minimax(b_copy, depth - 1, alpha, beta, True, opponent_piece, use_pruning)[1]\n",
    "            if new_score < value:\n",
    "                value = new_score\n",
    "                best_col = col\n",
    "            if use_pruning:\n",
    "                beta = min(beta, value)\n",
    "                if alpha >= beta:\n",
    "                    break\n",
    "        return best_col, value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente TD Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parámetros del juego y del algoritmo Q-learning\n",
    "ROW_COUNT = 6\n",
    "COLUMN_COUNT = 7\n",
    "EMPTY = 0\n",
    "\n",
    "# Parámetros del Q-learning (valores por defecto)\n",
    "ALPHA = 0.001       # Tasa de aprendizaje (para el optimizador de la red)\n",
    "GAMMA = 0.95        # Factor de descuento\n",
    "EPSILON = 1.0       # Tasa de exploración inicial\n",
    "EPSILON_MIN = 0.1   # Valor mínimo de epsilon\n",
    "EPSILON_DECAY = 0.995  # Factor de decaimiento de epsilon por episodio\n",
    "\n",
    "# Parámetros del modelo\n",
    "INPUT_SIZE = ROW_COUNT * COLUMN_COUNT * 3  # 126, por la representación one-hot\n",
    "OUTPUT_SIZE = COLUMN_COUNT  # 7 acciones (columnas)\n",
    "\n",
    "# ---------------- Modelo de Red Neuronal ---------------- #\n",
    "def build_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(INPUT_SIZE,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(OUTPUT_SIZE, activation='linear')  # Q-values para cada acción\n",
    "    ])\n",
    "    model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=ALPHA))\n",
    "    return model\n",
    "\n",
    "q_model = build_model()  \n",
    "# ---------------- Agente TD Learning con Modelo ---------------- #\n",
    "def choose_action(state, board, epsilon):\n",
    "    \"\"\"\n",
    "    Selecciona una acción usando política ε-greedy:\n",
    "      - Con probabilidad epsilon, elige una acción aleatoria de las válidas.\n",
    "      - Con probabilidad (1 - epsilon), elige la acción con mayor Q-value (filtrado por válidas).\n",
    "    \"\"\"\n",
    "    valid_actions = get_valid_locations(board)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(valid_actions)\n",
    "    state_input = state.reshape(1, INPUT_SIZE)\n",
    "    q_values = q_model.predict(state_input, verbose=0)[0]\n",
    "    q_valid = {action: q_values[action] for action in valid_actions}\n",
    "    return max(q_valid, key=q_valid.get)\n",
    "\n",
    "def update_Q(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    Actualiza el modelo usando un paso de Q-learning:\n",
    "      - Calcula el target Q-value para la acción tomada.\n",
    "      - Entrena la red para minimizar el error entre el Q-value predicho y el target.\n",
    "    \"\"\"\n",
    "    state_input = state.reshape(1, INPUT_SIZE)\n",
    "    next_state_input = next_state.reshape(1, INPUT_SIZE)\n",
    "    q_values = q_model.predict(state_input, verbose=0)\n",
    "    q_next = q_model.predict(next_state_input, verbose=0)\n",
    "    target = q_values.copy()\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + GAMMA * np.max(q_next)\n",
    "    q_model.fit(state_input, target, epochs=1, verbose=0)\n",
    "\n",
    "#The above code defines a function `simulate_game_td_vs_minimax` that simulates a game between agents using a combination of Temporal Difference Learning (TD) and Minimax algorithms. The function takes two parameters: `use_pruning` which is a boolean flag to indicate whether pruning should be used in the Minimax algorithm, and `depth` which specifies the depth of the Minimax search tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ciclo de Entrenamiento\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------- Ciclo de Entrenamiento ---------------- #\n",
    "def train_agent(episodes):\n",
    "    global EPSILON, current_board\n",
    "    for ep in range(episodes):\n",
    "        board = create_board()\n",
    "        current_board = board\n",
    "        game_over = False\n",
    "        turn = 0\n",
    "        total_reward = 0\n",
    "        while not game_over:\n",
    "            state = get_state_one_hot(board)\n",
    "            valid_actions = get_valid_locations(board)\n",
    "            if len(valid_actions) == 0:\n",
    "                game_over = True\n",
    "                break\n",
    "            # En self-play, ambos jugadores usan la misma política.\n",
    "            action = choose_action(state, board, EPSILON)\n",
    "            row = get_next_open_row(board, action)\n",
    "            # Alterna turno: turno 0 -> pieza 1, turno 1 -> pieza 2\n",
    "            piece = 1 if turn == 0 else 2\n",
    "            drop_piece(board, row, action, piece)\n",
    "            # Estructura de recompensas:\n",
    "            # Si gana: +100 para la pieza 2 (agente) y -100 para la pieza 1.\n",
    "            # Si es empate: 0.\n",
    "            # Si no termina: -1 por movimiento.\n",
    "            if winning_move(board, piece):\n",
    "                reward = 100 if piece == 2 else -100\n",
    "                game_over = True\n",
    "            elif is_terminal_node(board):\n",
    "                reward = 0\n",
    "                game_over = True\n",
    "            else:\n",
    "                reward = -1\n",
    "            total_reward += reward\n",
    "            next_state = get_state_one_hot(board)\n",
    "            update_Q(state, action, reward, next_state, game_over)\n",
    "            turn = (turn + 1) % 2\n",
    "            current_board = board\n",
    "        if EPSILON > EPSILON_MIN:\n",
    "            EPSILON *= EPSILON_DECAY\n",
    "        print(f\"Episode {ep+1}/{episodes} - Total Reward: {total_reward} - Epsilon: {EPSILON:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulación de partida entre agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Simulación de Partidas entre Agentes ---------------- #\n",
    "def simulate_game_td_vs_minimax(use_pruning=False, depth=4):\n",
    "    \"\"\"\n",
    "    Simula una partida entre el agente TD Learning (pieza 2) y un agente Minimax (pieza 1).\n",
    "    - El agente TD Learning usa la política actual (sin exploración, epsilon=0).\n",
    "    - El agente Minimax usa el algoritmo minimax, con o sin poda según 'use_pruning'.\n",
    "    Devuelve: 2 si gana el TD, 1 si gana Minimax, 0 en empate.\n",
    "    \"\"\"\n",
    "    board = create_board()\n",
    "    game_over = False\n",
    "    turn = 0  # Turno 0: Minimax (pieza 1), Turno 1: TD Learning (pieza 2)\n",
    "    while not game_over:\n",
    "        if turn == 0:\n",
    "            # Agente Minimax\n",
    "            col, _ = minimax(board, depth, -math.inf, math.inf, True, 1, use_pruning)\n",
    "            if col is None:\n",
    "                return 0  # Empate\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 1)\n",
    "            if winning_move(board, 1):\n",
    "                return 1\n",
    "        else:\n",
    "            # Agente TD Learning (usamos epsilon 0 para evaluar la política aprendida)\n",
    "            state = get_state_one_hot(board)\n",
    "            col = choose_action(state, board, 0.0)\n",
    "            if col not in get_valid_locations(board):\n",
    "                col = random.choice(get_valid_locations(board))\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 2)\n",
    "            if winning_move(board, 2):\n",
    "                return 2\n",
    "        if is_terminal_node(board):\n",
    "            return 0  # Empate\n",
    "        turn = (turn + 1) % 2\n",
    "    return 0\n",
    "\n",
    "# ---------------- Modo de Evaluación: Jugar contra el agente entrenado ---------------- #\n",
    "def play_game():\n",
    "    board = create_board()\n",
    "    game_over = False\n",
    "    turn = 0\n",
    "    print_board(board)\n",
    "    while not game_over:\n",
    "        if turn == 0:\n",
    "            valid_cols = get_valid_locations(board)\n",
    "            col = -1\n",
    "            while col not in valid_cols:\n",
    "                try:\n",
    "                    col = int(input(f\"Col (0-{COLUMN_COUNT-1}): \"))\n",
    "                    if col not in valid_cols:\n",
    "                        print(\"No válido, intente otra vez.\")\n",
    "                except ValueError:\n",
    "                    print(\"Entrada inválida.\")\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 1)\n",
    "            if winning_move(board, 1):\n",
    "                winning_pos = get_winning_positions(board, 1)\n",
    "                print_board(board, winning_pos)\n",
    "                print(\"¡Ganaste!\")\n",
    "                game_over = True\n",
    "            else:\n",
    "                print_board(board)\n",
    "        else:\n",
    "            state = get_state_one_hot(board)\n",
    "            col = choose_action(state, board, 0.0)  # Sin exploración\n",
    "            if col not in get_valid_locations(board):\n",
    "                col = random.choice(get_valid_locations(board))\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 2)\n",
    "            if winning_move(board, 2):\n",
    "                winning_pos = get_winning_positions(board, 2)\n",
    "                print_board(board, winning_pos)\n",
    "                print(\"IA gana.\")\n",
    "                game_over = True\n",
    "            else:\n",
    "                print_board(board)\n",
    "        turn = (turn + 1) % 2\n",
    "\n",
    "# ---------------- Modo de Evaluación: Jugar contra el agente entrenado ---------------- #\n",
    "def play_game():\n",
    "    board = create_board()\n",
    "    game_over = False\n",
    "    turn = 0\n",
    "    print_board(board)\n",
    "    while not game_over:\n",
    "        if turn == 0:\n",
    "            valid_cols = get_valid_locations(board)\n",
    "            col = -1\n",
    "            while col not in valid_cols:\n",
    "                try:\n",
    "                    col = int(input(f\"Col (0-{COLUMN_COUNT-1}): \"))\n",
    "                    if col not in valid_cols:\n",
    "                        print(\"No válido, intente otra vez.\")\n",
    "                except ValueError:\n",
    "                    print(\"Entrada inválida.\")\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 1)\n",
    "            if winning_move(board, 1):\n",
    "                winning_pos = get_winning_positions(board, 1)\n",
    "                print_board(board, winning_pos)\n",
    "                print(\"¡Ganaste!\")\n",
    "                game_over = True\n",
    "            else:\n",
    "                print_board(board)\n",
    "        else:\n",
    "            state = get_state_one_hot(board)\n",
    "            col = choose_action(state, board, 0.0)  # Sin exploración\n",
    "            if col not in get_valid_locations(board):\n",
    "                col = random.choice(get_valid_locations(board))\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 2)\n",
    "            if winning_move(board, 2):\n",
    "                winning_pos = get_winning_positions(board, 2)\n",
    "                print_board(board, winning_pos)\n",
    "                print(\"IA gana.\")\n",
    "                game_over = True\n",
    "            else:\n",
    "                print_board(board)\n",
    "        turn = (turn + 1) % 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_window(window, piece):\n",
    "    score = 0\n",
    "    opponent_piece = 1 if piece == 2 else 2\n",
    "    if window.count(piece) == 4:\n",
    "        score += 100\n",
    "    elif window.count(piece) == 3 and window.count(EMPTY) == 1:\n",
    "        score += 5\n",
    "    elif window.count(piece) == 2 and window.count(EMPTY) == 2:\n",
    "        score += 2\n",
    "    if window.count(opponent_piece) == 3 and window.count(EMPTY) == 1:\n",
    "        score -= 4\n",
    "    return score\n",
    "\n",
    "def score_position(board, piece):\n",
    "    score = 0\n",
    "    # Centro del tablero es importante en Connect Four\n",
    "    center_column = [board[r][COLUMN_COUNT // 2] for r in range(ROW_COUNT)]\n",
    "    score += center_column.count(piece) * 3\n",
    "    # Evaluar horizontalmente\n",
    "    for r in range(ROW_COUNT):\n",
    "        row_array = board[r]\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            window = row_array[c : c + 4]\n",
    "            score += evaluate_window(window, piece)\n",
    "    # Evaluar verticalmente\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        col_array = [board[r][c] for r in range(ROW_COUNT)]\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            window = col_array[r : r + 4]\n",
    "            score += evaluate_window(window, piece)\n",
    "    # Evaluar diagonales\n",
    "    for r in range(ROW_COUNT - 3):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            window = [board[r+i][c+i] for i in range(4)]\n",
    "            score += evaluate_window(window, piece)\n",
    "    for r in range(3, ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            window = [board[r-i][c+i] for i in range(4)]\n",
    "            score += evaluate_window(window, piece)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluación y grafico de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados TD vs. Minimax (sin poda): {'TD Wins': 17, 'Minimax Wins': 33, 'Empates': 0}\n",
      "Resultados TD vs. Minimax (con poda): {'TD Wins': 11, 'Minimax+Poda Wins': 39, 'Empates': 0}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_agents(num_games=50, depth=4):\n",
    "    results_minimax = {\"TD Wins\": 0, \"Minimax Wins\": 0, \"Empates\": 0}\n",
    "    results_alpha_beta = {\"TD Wins\": 0, \"Minimax+Poda Wins\": 0, \"Empates\": 0}\n",
    "\n",
    "    # Simular juegos TD vs. Minimax sin poda\n",
    "    for _ in range(num_games):\n",
    "        result = simulate_game_td_vs_minimax(use_pruning=False, depth=depth)\n",
    "        if result == 2:\n",
    "            results_minimax[\"TD Wins\"] += 1\n",
    "        elif result == 1:\n",
    "            results_minimax[\"Minimax Wins\"] += 1\n",
    "        else:\n",
    "            results_minimax[\"Empates\"] += 1\n",
    "\n",
    "    # Simular juegos TD vs. Minimax con poda alpha-beta\n",
    "    for _ in range(num_games):\n",
    "        result = simulate_game_td_vs_minimax(use_pruning=True, depth=depth)\n",
    "        if result == 2:\n",
    "            results_alpha_beta[\"TD Wins\"] += 1\n",
    "        elif result == 1:\n",
    "            results_alpha_beta[\"Minimax+Poda Wins\"] += 1\n",
    "        else:\n",
    "            results_alpha_beta[\"Empates\"] += 1\n",
    "\n",
    "    return results_minimax, results_alpha_beta\n",
    "\n",
    "def plot_results(results_minimax, results_alpha_beta):\n",
    "    # Graficar resultados con matplotlib y guardar en PDF\n",
    "    import matplotlib.pyplot as plt  # Asegúrate de importar matplotlib\n",
    "    labels = list(results_minimax.keys())\n",
    "    values_minimax = [results_minimax[label] for label in labels]\n",
    "    values_alpha_beta = [results_alpha_beta.get(label, 0) for label in labels]\n",
    "\n",
    "    x = np.arange(len(labels))  # posición de las etiquetas\n",
    "    width = 0.35  # ancho de las barras\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, values_minimax, width, label='Minimax')\n",
    "    rects2 = ax.bar(x + width/2, values_alpha_beta, width, label='Minimax+Poda')\n",
    "\n",
    "    ax.set_ylabel('Número de Victorias')\n",
    "    ax.set_title('Resultados de 50 juegos: TD vs. Minimax y TD vs. Minimax+Poda')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    for rect in rects1 + rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 puntos de desplazamiento\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"resultados.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "# Función que evalúa y grafica automáticamente\n",
    "def evaluate_and_plot(num_games=50, depth=4):\n",
    "    results_minimax, results_alpha_beta = evaluate_agents(num_games, depth)\n",
    "    print(\"Resultados TD vs. Minimax (sin poda):\", results_minimax)\n",
    "    print(\"Resultados TD vs. Minimax (con poda):\", results_alpha_beta)\n",
    "    plot_results(results_minimax, results_alpha_beta)\n",
    "\n",
    "# Llamada directa para evaluar y graficar:\n",
    "evaluate_and_plot(num_games=50, depth=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Fine Tuning ---------------- #\n",
    "def fine_tuning():\n",
    "    global ALPHA, GAMMA, EPSILON, EPSILON_DECAY, q_model\n",
    "    print(\"Ajuste de parámetros del algoritmo TD Learning\")\n",
    "    new_alpha = input(f\"Ingrese la tasa de aprendizaje (ALPHA) actual ({ALPHA}): \").strip()\n",
    "    if new_alpha != \"\":\n",
    "        ALPHA = float(new_alpha)\n",
    "        # Reconstruir el modelo con el nuevo ALPHA\n",
    "        q_model = build_model()\n",
    "    new_gamma = input(f\"Ingrese el factor de descuento (GAMMA) actual ({GAMMA}): \").strip()\n",
    "    if new_gamma != \"\":\n",
    "        GAMMA = float(new_gamma)\n",
    "    new_epsilon = input(f\"Ingrese la tasa de exploración (EPSILON) actual ({EPSILON}): \").strip()\n",
    "    if new_epsilon != \"\":\n",
    "        EPSILON = float(new_epsilon)\n",
    "    new_decay = input(f\"Ingrese el factor de decaimiento (EPSILON_DECAY) actual ({EPSILON_DECAY}): \").strip()\n",
    "    if new_decay != \"\":\n",
    "        EPSILON_DECAY = float(new_decay)\n",
    "    print(\"Nuevos parámetros:\")\n",
    "    print(f\"ALPHA = {ALPHA}, GAMMA = {GAMMA}, EPSILON = {EPSILON}, EPSILON_DECAY = {EPSILON_DECAY}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menu de ejecucion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Connect 4 con TD Learning ===\n",
      "1. Entrenar agente\n",
      "2. Jugar contra el agente entrenado\n",
      "3. Evaluar agente automáticamente\n",
      "4. Ajustar parámetros\n",
      "5. Salir\n",
      "Episode 1/2 - Total Reward: 89 - Epsilon: 0.995\n",
      "Episode 2/2 - Total Reward: 75 - Epsilon: 0.990\n",
      "=== Connect 4 con TD Learning ===\n",
      "1. Entrenar agente\n",
      "2. Jugar contra el agente entrenado\n",
      "3. Evaluar agente automáticamente\n",
      "4. Ajustar parámetros\n",
      "5. Salir\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 1 0 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 2 0 \n",
      "0 0 0 0 0 1 0 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 2 0 \n",
      "0 1 0 0 0 1 0 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 2 0 \n",
      "0 1 2 0 0 1 0 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 1 0 0 2 0 \n",
      "0 1 2 0 0 1 0 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 1 0 0 2 0 \n",
      "0 1 2 0 0 1 0 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 1 0 0 2 0 \n",
      "0 1 2 0 1 1 0 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 1 0 0 2 0 \n",
      "0 1 2 0 1 1 0 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 1 0 0 2 0 \n",
      "0 1 2 0 1 1 1 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 1 0 0 2 0 \n",
      "0 1 2 0 1 1 1 \n",
      "\n",
      "0 0 0 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 2 0 0 0 0 \n",
      "0 0 1 0 1 2 0 \n",
      "0 1 2 0 1 1 1 \n",
      "\n",
      "0 0 \u001b[91m2\u001b[0m 0 0 0 0 \n",
      "0 0 \u001b[91m2\u001b[0m 0 0 0 0 \n",
      "0 0 \u001b[91m2\u001b[0m 0 0 0 0 \n",
      "0 0 \u001b[91m2\u001b[0m 0 0 0 0 \n",
      "0 0 1 0 1 2 0 \n",
      "0 1 2 0 1 1 1 \n",
      "\n",
      "IA gana.\n",
      "=== Connect 4 con TD Learning ===\n",
      "1. Entrenar agente\n",
      "2. Jugar contra el agente entrenado\n",
      "3. Evaluar agente automáticamente\n",
      "4. Ajustar parámetros\n",
      "5. Salir\n",
      "Ajuste de parámetros del algoritmo TD Learning\n",
      "Nuevos parámetros:\n",
      "ALPHA = 4.0, GAMMA = 1.0, EPSILON = 1.0, EPSILON_DECAY = 2.0\n",
      "=== Connect 4 con TD Learning ===\n",
      "1. Entrenar agente\n",
      "2. Jugar contra el agente entrenado\n",
      "3. Evaluar agente automáticamente\n",
      "4. Ajustar parámetros\n",
      "5. Salir\n",
      "¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ejecutar():\n",
    "    print(\"=== Connect 4 con TD Learning ===\")\n",
    "    print(\"1. Entrenar agente\")\n",
    "    print(\"2. Jugar contra el agente entrenado\")\n",
    "    print(\"3. Evaluar agente automáticamente\")\n",
    "    print(\"4. Ajustar parámetros\")\n",
    "    print(\"5. Salir\")\n",
    "    \n",
    "    opcion = input(\"Seleccione una opción (1-5): \").strip().lower()\n",
    "    \n",
    "    if opcion == \"1\":\n",
    "        episodios = int(input(\"Número de episodios de entrenamiento: \"))\n",
    "        train_agent(episodios)\n",
    "        ejecutar()\n",
    "    elif opcion == \"2\":\n",
    "        play_game()\n",
    "        ejecutar()\n",
    "    elif opcion == \"3\":\n",
    "        evaluate_agents_automatic(depth=3)\n",
    "        ejecutar()\n",
    "    elif opcion == \"4\":\n",
    "        fine_tuning()\n",
    "        ejecutar()\n",
    "    elif opcion == \"5\":\n",
    "        print(\"¡Hasta luego!\")\n",
    "    else:\n",
    "        print(\"Opción no válida. Intente de nuevo.\")\n",
    "        ejecutar()\n",
    "\n",
    "ejecutar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juego de suma cero  Gana o pierda todo  Presentar juegos de suma cero en la teoria del juego - FasterCapital. (s. f.). FasterCapital. https://fastercapital.com/es/contenido/Juego-de-suma-cero--Gana-o-pierda-todo--Presentar-juegos-de-suma-cero-en-la-teoria-del-juego.html\n",
    "\n",
    "- Alex. (s. f.). Teoría de juegos: Estrategia y toma de decisiones en situaciones competitivas. AproximadaMente. Pensar Mejor y Tomar Mejores Decisiones. https://www.aproximadamente.com/teoria-de-juegos%3A-estrategia-y-toma-de-decisiones-en-situaciones-competitivas/\n",
    "\n",
    "- How can you use temporal difference learning in a reinforcement learning project? (2024, 8 enero). https://www.linkedin.com/advice/0/how-can-you-use-temporal-difference-learning-99ncc\n",
    "\n",
    "- Cerretani, J. (2024, 20 diciembre). Aprendizaje por refuerzo (RL) — Capítulo 1: Historia del aprendizaje por refuerzo — Parte 3: Aprendizaje por diferencia temporal. Medium. https://medium.com/%40joancerretanids/aprendizaje-por-refuerzo-rl-cap%C3%ADtulo-1-historia-del-aprendizaje-por-refuerzo-parte-3-fc4d17197680\n",
    "\n",
    "- Libretexts. (2022, 2 noviembre). 4: Juegos que no son de suma cero. LibreTexts Español. https://espanol.libretexts.org/Matematicas/Matematicas_Aplicadas/Introduccion_a_la_teoria_de_juegos%3A_un_enfoque_de_descubrimiento_%28Nordstrom%29/04%3A_Juegos_que_no_son_de_suma_cero\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
