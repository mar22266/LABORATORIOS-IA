{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "### Sergio Orellana - 221122\n",
    "\n",
    "### Andre Marroquin - 22266\n",
    "\n",
    "### Rodrigo Mansilla - 22611\n",
    "\n",
    "# Link del repositorio\n",
    "\n",
    "https://github.com/mar22266/LABORATORIOS-IA.git\n",
    "\n",
    "# Link del video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste de parámetros del algoritmo TD Learning\n",
      "Nuevos parámetros:\n",
      "ALPHA = 0.005, GAMMA = 0.9, EPSILON = 1.0, EPSILON_DECAY = 0.995\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "# Parámetros del juego y del algoritmo Q-learning\n",
    "ROW_COUNT = 6\n",
    "COLUMN_COUNT = 7\n",
    "EMPTY = 0\n",
    "\n",
    "# Parámetros del Q-learning (valores por defecto)\n",
    "ALPHA = 0.001       # Tasa de aprendizaje (para el optimizador de la red)\n",
    "GAMMA = 0.95        # Factor de descuento\n",
    "EPSILON = 1.0       # Tasa de exploración inicial\n",
    "EPSILON_MIN = 0.1   # Valor mínimo de epsilon\n",
    "EPSILON_DECAY = 0.995  # Factor de decaimiento de epsilon por episodio\n",
    "\n",
    "# Parámetros del modelo\n",
    "INPUT_SIZE = ROW_COUNT * COLUMN_COUNT * 3  # 126, por la representación one-hot\n",
    "OUTPUT_SIZE = COLUMN_COUNT  # 7 acciones (columnas)\n",
    "\n",
    "# ---------------- Modelo de Red Neuronal ---------------- #\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, input_dim=INPUT_SIZE, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(OUTPUT_SIZE, activation='linear'))  # Q-values para cada acción\n",
    "    model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=ALPHA))\n",
    "    return model\n",
    "\n",
    "q_model = build_model()  # Modelo global\n",
    "\n",
    "# ---------------- Funciones del juego ---------------- #\n",
    "def create_board():\n",
    "    return [[EMPTY for _ in range(COLUMN_COUNT)] for _ in range(ROW_COUNT)]\n",
    "\n",
    "def drop_piece(board, row, col, piece):\n",
    "    board[row][col] = piece\n",
    "\n",
    "def is_valid_location(board, col):\n",
    "    return board[0][col] == EMPTY\n",
    "\n",
    "def get_next_open_row(board, col):\n",
    "    for r in range(ROW_COUNT - 1, -1, -1):\n",
    "        if board[r][col] == EMPTY:\n",
    "            return r\n",
    "    return None\n",
    "\n",
    "def winning_move(board, piece):\n",
    "    # Horizontal\n",
    "    for r in range(ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r][c+1] == piece and \n",
    "                board[r][c+2] == piece and board[r][c+3] == piece):\n",
    "                return True\n",
    "    # Vertical\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c] == piece and \n",
    "                board[r+2][c] == piece and board[r+3][c] == piece):\n",
    "                return True\n",
    "    # Diagonal positiva\n",
    "    for r in range(3, ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r-1][c+1] == piece and \n",
    "                board[r-2][c+2] == piece and board[r-3][c+3] == piece):\n",
    "                return True\n",
    "    # Diagonal negativa\n",
    "    for r in range(ROW_COUNT - 3):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c+1] == piece and \n",
    "                board[r+2][c+2] == piece and board[r+3][c+3] == piece):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_winning_positions(board, piece):\n",
    "    for r in range(ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r][c+1] == piece and \n",
    "                board[r][c+2] == piece and board[r][c+3] == piece):\n",
    "                return [(r, c + i) for i in range(4)]\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c] == piece and \n",
    "                board[r+2][c] == piece and board[r+3][c] == piece):\n",
    "                return [(r + i, c) for i in range(4)]\n",
    "    for r in range(3, ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r-1][c+1] == piece and \n",
    "                board[r-2][c+2] == piece and board[r-3][c+3] == piece):\n",
    "                return [(r - i, c + i) for i in range(4)]\n",
    "    for r in range(ROW_COUNT - 3):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c+1] == piece and \n",
    "                board[r+2][c+2] == piece and board[r+3][c+3] == piece):\n",
    "                return [(r + i, c + i) for i in range(4)]\n",
    "    return []\n",
    "\n",
    "def get_valid_locations(board):\n",
    "    valid = []\n",
    "    for col in range(COLUMN_COUNT):\n",
    "        if is_valid_location(board, col):\n",
    "            valid.append(col)\n",
    "    return valid\n",
    "\n",
    "def is_terminal_node(board):\n",
    "    return winning_move(board, 1) or winning_move(board, 2) or len(get_valid_locations(board)) == 0\n",
    "\n",
    "# Representación del estado: one-hot\n",
    "def get_state_one_hot(board):\n",
    "    mapping = {0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]}\n",
    "    state = []\n",
    "    for row in board:\n",
    "        for cell in row:\n",
    "            state.extend(mapping[cell])\n",
    "    return np.array(state)\n",
    "\n",
    "def print_board(board, winning_positions=[]):\n",
    "    for r in range(ROW_COUNT):\n",
    "        row_str = \"\"\n",
    "        for c in range(COLUMN_COUNT):\n",
    "            cell = board[r][c]\n",
    "            if (r, c) in winning_positions:\n",
    "                row_str += \"\\033[91m\" + str(cell) + \"\\033[0m\" + \" \"\n",
    "            else:\n",
    "                row_str += str(cell) + \" \"\n",
    "        print(row_str)\n",
    "    print(\"\")\n",
    "\n",
    "# ---------------- Agente TD Learning con Modelo ---------------- #\n",
    "def choose_action(state, board, epsilon):\n",
    "    \"\"\"\n",
    "    Selecciona una acción usando política ε-greedy:\n",
    "      - Con probabilidad epsilon, elige una acción aleatoria de las válidas.\n",
    "      - Con probabilidad (1 - epsilon), elige la acción con mayor Q-value (filtrado por válidas).\n",
    "    \"\"\"\n",
    "    valid_actions = get_valid_locations(board)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(valid_actions)\n",
    "    state_input = state.reshape(1, INPUT_SIZE)\n",
    "    q_values = q_model.predict(state_input, verbose=0)[0]\n",
    "    q_valid = {action: q_values[action] for action in valid_actions}\n",
    "    return max(q_valid, key=q_valid.get)\n",
    "\n",
    "def update_Q(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    Actualiza el modelo usando un paso de Q-learning:\n",
    "      - Calcula el target Q-value para la acción tomada.\n",
    "      - Entrena la red para minimizar el error entre el Q-value predicho y el target.\n",
    "    \"\"\"\n",
    "    state_input = state.reshape(1, INPUT_SIZE)\n",
    "    next_state_input = next_state.reshape(1, INPUT_SIZE)\n",
    "    q_values = q_model.predict(state_input, verbose=0)\n",
    "    q_next = q_model.predict(next_state_input, verbose=0)\n",
    "    target = q_values.copy()\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + GAMMA * np.max(q_next)\n",
    "    q_model.fit(state_input, target, epochs=1, verbose=0)\n",
    "\n",
    "# ---------------- Ciclo de Entrenamiento ---------------- #\n",
    "def train_agent(episodes):\n",
    "    global EPSILON, current_board\n",
    "    for ep in range(episodes):\n",
    "        board = create_board()\n",
    "        current_board = board\n",
    "        game_over = False\n",
    "        turn = 0\n",
    "        total_reward = 0\n",
    "        while not game_over:\n",
    "            state = get_state_one_hot(board)\n",
    "            valid_actions = get_valid_locations(board)\n",
    "            if len(valid_actions) == 0:\n",
    "                game_over = True\n",
    "                break\n",
    "            # En self-play, ambos jugadores usan la misma política.\n",
    "            action = choose_action(state, board, EPSILON)\n",
    "            row = get_next_open_row(board, action)\n",
    "            # Alterna turno: turno 0 -> pieza 1, turno 1 -> pieza 2\n",
    "            piece = 1 if turn == 0 else 2\n",
    "            drop_piece(board, row, action, piece)\n",
    "            # Estructura de recompensas:\n",
    "            # Si gana: +100 para la pieza 2 (agente) y -100 para la pieza 1.\n",
    "            # Si es empate: 0.\n",
    "            # Si no termina: -1 por movimiento.\n",
    "            if winning_move(board, piece):\n",
    "                reward = 100 if piece == 2 else -100\n",
    "                game_over = True\n",
    "            elif is_terminal_node(board):\n",
    "                reward = 0\n",
    "                game_over = True\n",
    "            else:\n",
    "                reward = -1\n",
    "            total_reward += reward\n",
    "            next_state = get_state_one_hot(board)\n",
    "            update_Q(state, action, reward, next_state, game_over)\n",
    "            turn = (turn + 1) % 2\n",
    "            current_board = board\n",
    "        if EPSILON > EPSILON_MIN:\n",
    "            EPSILON *= EPSILON_DECAY\n",
    "        print(f\"Episode {ep+1}/{episodes} - Total Reward: {total_reward} - Epsilon: {EPSILON:.3f}\")\n",
    "\n",
    "# ---------------- Fine Tuning ---------------- #\n",
    "def fine_tuning():\n",
    "    global ALPHA, GAMMA, EPSILON, EPSILON_DECAY, q_model\n",
    "    print(\"Ajuste de parámetros del algoritmo TD Learning\")\n",
    "    new_alpha = input(f\"Ingrese la tasa de aprendizaje (ALPHA) actual ({ALPHA}): \").strip()\n",
    "    if new_alpha != \"\":\n",
    "        ALPHA = float(new_alpha)\n",
    "        # Reconstruir el modelo con el nuevo ALPHA\n",
    "        q_model = build_model()\n",
    "    new_gamma = input(f\"Ingrese el factor de descuento (GAMMA) actual ({GAMMA}): \").strip()\n",
    "    if new_gamma != \"\":\n",
    "        GAMMA = float(new_gamma)\n",
    "    new_epsilon = input(f\"Ingrese la tasa de exploración (EPSILON) actual ({EPSILON}): \").strip()\n",
    "    if new_epsilon != \"\":\n",
    "        EPSILON = float(new_epsilon)\n",
    "    new_decay = input(f\"Ingrese el factor de decaimiento (EPSILON_DECAY) actual ({EPSILON_DECAY}): \").strip()\n",
    "    if new_decay != \"\":\n",
    "        EPSILON_DECAY = float(new_decay)\n",
    "    print(\"Nuevos parámetros:\")\n",
    "    print(f\"ALPHA = {ALPHA}, GAMMA = {GAMMA}, EPSILON = {EPSILON}, EPSILON_DECAY = {EPSILON_DECAY}\")\n",
    "\n",
    "# ---------------- Modo de Evaluación: Jugar contra el agente entrenado ---------------- #\n",
    "def play_game():\n",
    "    board = create_board()\n",
    "    game_over = False\n",
    "    turn = 0\n",
    "    print_board(board)\n",
    "    while not game_over:\n",
    "        if turn == 0:\n",
    "            valid_cols = get_valid_locations(board)\n",
    "            col = -1\n",
    "            while col not in valid_cols:\n",
    "                try:\n",
    "                    col = int(input(f\"Col (0-{COLUMN_COUNT-1}): \"))\n",
    "                    if col not in valid_cols:\n",
    "                        print(\"No válido, intente otra vez.\")\n",
    "                except ValueError:\n",
    "                    print(\"Entrada inválida.\")\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 1)\n",
    "            if winning_move(board, 1):\n",
    "                winning_pos = get_winning_positions(board, 1)\n",
    "                print_board(board, winning_pos)\n",
    "                print(\"¡Ganaste!\")\n",
    "                game_over = True\n",
    "            else:\n",
    "                print_board(board)\n",
    "        else:\n",
    "            state = get_state_one_hot(board)\n",
    "            col = choose_action(state, board, 0.0)  # Sin exploración\n",
    "            if col not in get_valid_locations(board):\n",
    "                col = random.choice(get_valid_locations(board))\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 2)\n",
    "            if winning_move(board, 2):\n",
    "                winning_pos = get_winning_positions(board, 2)\n",
    "                print_board(board, winning_pos)\n",
    "                print(\"IA gana.\")\n",
    "                game_over = True\n",
    "            else:\n",
    "                print_board(board)\n",
    "        turn = (turn + 1) % 2\n",
    "\n",
    "# ---------------- Opciones de Ejecución ---------------- #\n",
    "def ejecutar():\n",
    "    mode_input = input(\"Elija 'train' para entrenamiento, 'play' para jugar, o 'tune' para ajustar parámetros: \").strip().lower()\n",
    "    if mode_input == \"train\":\n",
    "        episodes = int(input(\"Ingrese el número de episodios para entrenar: \"))\n",
    "        train_agent(episodes)\n",
    "    elif mode_input == \"play\":\n",
    "        play_game()\n",
    "    elif mode_input == \"tune\":\n",
    "        fine_tuning()\n",
    "    else:\n",
    "        print(\"Opción no válida.\")\n",
    "\n",
    "# Variable global para acceso al tablero actual en choose_action (solo para este ejemplo)\n",
    "current_board = None\n",
    "\n",
    "ejecutar()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
