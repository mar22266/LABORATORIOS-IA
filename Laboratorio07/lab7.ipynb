{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "### Sergio Orellana - 221122\n",
    "\n",
    "### Andre Marroquin - 22266\n",
    "\n",
    "### Rodrigo Mansilla - 22611\n",
    "\n",
    "# Link del repositorio\n",
    "\n",
    "https://github.com/mar22266/LABORATORIOS-IA.git\n",
    "\n",
    "# Link del video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es el aprendizaje por diferencias temporales (Temporal Difference Learning) y en qué se diferencia de los métodos tradicionales de aprendizaje supervisado? Explique el concepto de \"error de diferencia temporal\" y su papel en los algoritmos de aprendizaje por refuerzo.\n",
    "\n",
    "El aprendizaje por diferencias temporales TD es una técnica de aprendizaje por refuerzo que permite a un agente aprender a predecir valores futuros basándose en experiencias pasadas, sin necesidad de esperar una señal de recompensa final. A diferencia del aprendizaje supervisado, que requiere un conjunto de datos etiquetados para entrenar un modelo, el aprendizaje TD ajusta sus estimaciones de valor en función de la diferencia entre predicciones sucesivas a lo largo del tiempo.\n",
    "\n",
    "El \"error de diferencia temporal\" es la discrepancia entre la recompensa observada más el valor estimado del siguiente estado y el valor estimado del estado actual. Este error guía la actualización de las estimaciones de valor del agente, permitiéndole mejorar sus predicciones y decisiones futuras.\n",
    "\n",
    "\n",
    "2. En el contexto de los juegos simultáneos, ¿cómo toman decisiones los jugadores sin conocer las acciones de sus oponentes? Dé un ejemplo de un escenario del mundo real que pueda modelarse como un juego simultáneo y discuta las estrategias que los jugadores podrían emplear en tal situación.\n",
    "\n",
    "En los juegos simultáneos, los jugadores deben elegir sus estrategias al mismo tiempo, sin información sobre las elecciones de los demás. Para tomar decisiones, cada jugador considera las posibles acciones de los oponentes y evalúa las mejores respuestas a esas acciones, utilizando conceptos como el equilibrio de Nash para identificar estrategias óptimas.\n",
    "\n",
    "Un ejemplo del mundo real es la competencia entre empresas que lanzan productos similares al mercado. Sin conocer las decisiones exactas de sus competidores, cada empresa debe decidir aspectos como el precio, la calidad y las características del producto. Las estrategias pueden incluir la diferenciación del producto, la competencia en precios o la innovación para captar una mayor cuota de mercado.\n",
    "\n",
    "\n",
    "3. ¿Qué distingue los juegos de suma cero de los juegos de no suma cero y cómo afecta esta diferencia al proceso de toma de decisiones de los jugadores? Proporcione al menos un ejemplo de juegos que entren en la categoría de juegos de no suma cero y discuta las consideraciones estratégicas únicas involucradas.\n",
    "\n",
    "En los juegos de suma cero, la ganancia de un jugador es exactamente igual a la pérdida de otro; es decir, el total de ganancias y pérdidas suma cero. En contraste, en los juegos de no suma cero, es posible que todos los jugadores ganen o pierdan simultáneamente, permitiendo resultados de beneficio mutuo o perjuicio compartido.\n",
    "\n",
    "Esta diferencia influye en la toma de decisiones: en juegos de suma cero, los jugadores suelen adoptar estrategias competitivas enfocadas en maximizar su propia ganancia a expensas del oponente. En juegos de no suma cero, las estrategias pueden ser más cooperativas, buscando soluciones que beneficien a todas las partes involucradas.\n",
    "\n",
    "Un ejemplo de juego de no suma cero es el dilema del prisionero, donde dos sospechosos deciden independientemente si confesar o no. La cooperación no confesar puede resultar mejor para ambos, mientras que la traición mutua ambos confiesan lleva a resultados peores. Este escenario destaca la tensión entre la cooperación y la competencia en la toma de decisiones estratégicas.\n",
    "\n",
    "\n",
    "4. ¿Cómo se aplica el concepto de equilibrio de Nash a los juegos simultáneos? Explique cómo el equilibrio de Nash representa una solución estable en la que ningún jugador tiene un incentivo para desviarse unilateralmente de la estrategia elegida.\n",
    "\n",
    "El equilibrio de Nash en juegos simultáneos es una situación en la que cada jugador adopta la mejor estrategia posible, considerando las estrategias elegidas por los demás jugadores. En este equilibrio, ningún jugador puede mejorar su resultado cambiando unilateralmente su estrategia, lo que lo convierte en una solución estable.\n",
    "\n",
    "Este concepto es fundamental en la teoría de juegos, ya que permite predecir resultados en situaciones donde los participantes toman decisiones interdependientes. En un equilibrio de Nash, cada jugador reconoce que cualquier desviación unilateral no le proporcionará un beneficio adicional, lo que fomenta la estabilidad de las estrategias adoptadas.\n",
    "\n",
    "\n",
    "5. Discuta la aplicación del aprendizaje por diferencias temporales en el modelado y optimización de procesos de toma de decisiones en entornos dinámicos. ¿Cómo maneja el aprendizaje por diferencias temporales el equilibrio entre exploración y explotación y cuáles son algunos de los desafíos asociados con su implementación en la práctica?\n",
    "\n",
    "El aprendizaje por diferencias temporales (TD) se aplica en entornos dinámicos para modelar y optimizar procesos de toma de decisiones, permitiendo a un agente aprender políticas óptimas a través de la interacción continua con el entorno. Al actualizar las estimaciones de valor basándose en experiencias recientes, el aprendizaje TD es eficaz en situaciones donde el modelo del entorno es desconocido o cambia con el tiempo.\n",
    "\n",
    "Para equilibrar la exploración, descubrir nuevas estrategias y la explotación utilizar estrategias conocidas que ofrecen recompensas, se emplean políticas como e-greedy, donde el agente elige acciones aleatorias con una pequeña probabilidad e y, en otros casos, selecciona la acción que maximiza la recompensa esperada.\n",
    "\n",
    "Los desafíos en la implementación práctica del aprendizaje TD incluyen la selección adecuada de la tasa de aprendizaje y el manejo de la maldición de la dimensionalidad, que se refiere al aumento exponencial de la complejidad a medida que crece el espacio de estados y acciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajuste de parámetros del algoritmo TD Learning\n",
      "Nuevos parámetros:\n",
      "ALPHA = 0.005, GAMMA = 0.9, EPSILON = 1.0, EPSILON_DECAY = 0.995\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "# Parámetros del juego y del algoritmo Q-learning\n",
    "ROW_COUNT = 6\n",
    "COLUMN_COUNT = 7\n",
    "EMPTY = 0\n",
    "\n",
    "# Parámetros del Q-learning (valores por defecto)\n",
    "ALPHA = 0.001       # Tasa de aprendizaje (para el optimizador de la red)\n",
    "GAMMA = 0.95        # Factor de descuento\n",
    "EPSILON = 1.0       # Tasa de exploración inicial\n",
    "EPSILON_MIN = 0.1   # Valor mínimo de epsilon\n",
    "EPSILON_DECAY = 0.995  # Factor de decaimiento de epsilon por episodio\n",
    "\n",
    "# Parámetros del modelo\n",
    "INPUT_SIZE = ROW_COUNT * COLUMN_COUNT * 3  # 126, por la representación one-hot\n",
    "OUTPUT_SIZE = COLUMN_COUNT  # 7 acciones (columnas)\n",
    "\n",
    "# ---------------- Modelo de Red Neuronal ---------------- #\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, input_dim=INPUT_SIZE, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(OUTPUT_SIZE, activation='linear'))  # Q-values para cada acción\n",
    "    model.compile(loss='mse', optimizer=optimizers.Adam(learning_rate=ALPHA))\n",
    "    return model\n",
    "\n",
    "q_model = build_model()  # Modelo global\n",
    "\n",
    "# ---------------- Funciones del juego ---------------- #\n",
    "def create_board():\n",
    "    return [[EMPTY for _ in range(COLUMN_COUNT)] for _ in range(ROW_COUNT)]\n",
    "\n",
    "def drop_piece(board, row, col, piece):\n",
    "    board[row][col] = piece\n",
    "\n",
    "def is_valid_location(board, col):\n",
    "    return board[0][col] == EMPTY\n",
    "\n",
    "def get_next_open_row(board, col):\n",
    "    for r in range(ROW_COUNT - 1, -1, -1):\n",
    "        if board[r][col] == EMPTY:\n",
    "            return r\n",
    "    return None\n",
    "\n",
    "def winning_move(board, piece):\n",
    "    # Horizontal\n",
    "    for r in range(ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r][c+1] == piece and \n",
    "                board[r][c+2] == piece and board[r][c+3] == piece):\n",
    "                return True\n",
    "    # Vertical\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c] == piece and \n",
    "                board[r+2][c] == piece and board[r+3][c] == piece):\n",
    "                return True\n",
    "    # Diagonal positiva\n",
    "    for r in range(3, ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r-1][c+1] == piece and \n",
    "                board[r-2][c+2] == piece and board[r-3][c+3] == piece):\n",
    "                return True\n",
    "    # Diagonal negativa\n",
    "    for r in range(ROW_COUNT - 3):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c+1] == piece and \n",
    "                board[r+2][c+2] == piece and board[r+3][c+3] == piece):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def get_winning_positions(board, piece):\n",
    "    for r in range(ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r][c+1] == piece and \n",
    "                board[r][c+2] == piece and board[r][c+3] == piece):\n",
    "                return [(r, c + i) for i in range(4)]\n",
    "    for c in range(COLUMN_COUNT):\n",
    "        for r in range(ROW_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c] == piece and \n",
    "                board[r+2][c] == piece and board[r+3][c] == piece):\n",
    "                return [(r + i, c) for i in range(4)]\n",
    "    for r in range(3, ROW_COUNT):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r-1][c+1] == piece and \n",
    "                board[r-2][c+2] == piece and board[r-3][c+3] == piece):\n",
    "                return [(r - i, c + i) for i in range(4)]\n",
    "    for r in range(ROW_COUNT - 3):\n",
    "        for c in range(COLUMN_COUNT - 3):\n",
    "            if (board[r][c] == piece and board[r+1][c+1] == piece and \n",
    "                board[r+2][c+2] == piece and board[r+3][c+3] == piece):\n",
    "                return [(r + i, c + i) for i in range(4)]\n",
    "    return []\n",
    "\n",
    "def get_valid_locations(board):\n",
    "    valid = []\n",
    "    for col in range(COLUMN_COUNT):\n",
    "        if is_valid_location(board, col):\n",
    "            valid.append(col)\n",
    "    return valid\n",
    "\n",
    "def is_terminal_node(board):\n",
    "    return winning_move(board, 1) or winning_move(board, 2) or len(get_valid_locations(board)) == 0\n",
    "\n",
    "# Representación del estado: one-hot\n",
    "def get_state_one_hot(board):\n",
    "    mapping = {0: [1, 0, 0], 1: [0, 1, 0], 2: [0, 0, 1]}\n",
    "    state = []\n",
    "    for row in board:\n",
    "        for cell in row:\n",
    "            state.extend(mapping[cell])\n",
    "    return np.array(state)\n",
    "\n",
    "def print_board(board, winning_positions=[]):\n",
    "    for r in range(ROW_COUNT):\n",
    "        row_str = \"\"\n",
    "        for c in range(COLUMN_COUNT):\n",
    "            cell = board[r][c]\n",
    "            if (r, c) in winning_positions:\n",
    "                row_str += \"\\033[91m\" + str(cell) + \"\\033[0m\" + \" \"\n",
    "            else:\n",
    "                row_str += str(cell) + \" \"\n",
    "        print(row_str)\n",
    "    print(\"\")\n",
    "\n",
    "# ---------------- Agente TD Learning con Modelo ---------------- #\n",
    "def choose_action(state, board, epsilon):\n",
    "    \"\"\"\n",
    "    Selecciona una acción usando política ε-greedy:\n",
    "      - Con probabilidad epsilon, elige una acción aleatoria de las válidas.\n",
    "      - Con probabilidad (1 - epsilon), elige la acción con mayor Q-value (filtrado por válidas).\n",
    "    \"\"\"\n",
    "    valid_actions = get_valid_locations(board)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(valid_actions)\n",
    "    state_input = state.reshape(1, INPUT_SIZE)\n",
    "    q_values = q_model.predict(state_input, verbose=0)[0]\n",
    "    q_valid = {action: q_values[action] for action in valid_actions}\n",
    "    return max(q_valid, key=q_valid.get)\n",
    "\n",
    "def update_Q(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    Actualiza el modelo usando un paso de Q-learning:\n",
    "      - Calcula el target Q-value para la acción tomada.\n",
    "      - Entrena la red para minimizar el error entre el Q-value predicho y el target.\n",
    "    \"\"\"\n",
    "    state_input = state.reshape(1, INPUT_SIZE)\n",
    "    next_state_input = next_state.reshape(1, INPUT_SIZE)\n",
    "    q_values = q_model.predict(state_input, verbose=0)\n",
    "    q_next = q_model.predict(next_state_input, verbose=0)\n",
    "    target = q_values.copy()\n",
    "    if done:\n",
    "        target[0][action] = reward\n",
    "    else:\n",
    "        target[0][action] = reward + GAMMA * np.max(q_next)\n",
    "    q_model.fit(state_input, target, epochs=1, verbose=0)\n",
    "\n",
    "# ---------------- Ciclo de Entrenamiento ---------------- #\n",
    "def train_agent(episodes):\n",
    "    global EPSILON, current_board\n",
    "    for ep in range(episodes):\n",
    "        board = create_board()\n",
    "        current_board = board\n",
    "        game_over = False\n",
    "        turn = 0\n",
    "        total_reward = 0\n",
    "        while not game_over:\n",
    "            state = get_state_one_hot(board)\n",
    "            valid_actions = get_valid_locations(board)\n",
    "            if len(valid_actions) == 0:\n",
    "                game_over = True\n",
    "                break\n",
    "            # En self-play, ambos jugadores usan la misma política.\n",
    "            action = choose_action(state, board, EPSILON)\n",
    "            row = get_next_open_row(board, action)\n",
    "            # Alterna turno: turno 0 -> pieza 1, turno 1 -> pieza 2\n",
    "            piece = 1 if turn == 0 else 2\n",
    "            drop_piece(board, row, action, piece)\n",
    "            # Estructura de recompensas:\n",
    "            # Si gana: +100 para la pieza 2 (agente) y -100 para la pieza 1.\n",
    "            # Si es empate: 0.\n",
    "            # Si no termina: -1 por movimiento.\n",
    "            if winning_move(board, piece):\n",
    "                reward = 100 if piece == 2 else -100\n",
    "                game_over = True\n",
    "            elif is_terminal_node(board):\n",
    "                reward = 0\n",
    "                game_over = True\n",
    "            else:\n",
    "                reward = -1\n",
    "            total_reward += reward\n",
    "            next_state = get_state_one_hot(board)\n",
    "            update_Q(state, action, reward, next_state, game_over)\n",
    "            turn = (turn + 1) % 2\n",
    "            current_board = board\n",
    "        if EPSILON > EPSILON_MIN:\n",
    "            EPSILON *= EPSILON_DECAY\n",
    "        print(f\"Episode {ep+1}/{episodes} - Total Reward: {total_reward} - Epsilon: {EPSILON:.3f}\")\n",
    "\n",
    "# ---------------- Fine Tuning ---------------- #\n",
    "def fine_tuning():\n",
    "    global ALPHA, GAMMA, EPSILON, EPSILON_DECAY, q_model\n",
    "    print(\"Ajuste de parámetros del algoritmo TD Learning\")\n",
    "    new_alpha = input(f\"Ingrese la tasa de aprendizaje (ALPHA) actual ({ALPHA}): \").strip()\n",
    "    if new_alpha != \"\":\n",
    "        ALPHA = float(new_alpha)\n",
    "        # Reconstruir el modelo con el nuevo ALPHA\n",
    "        q_model = build_model()\n",
    "    new_gamma = input(f\"Ingrese el factor de descuento (GAMMA) actual ({GAMMA}): \").strip()\n",
    "    if new_gamma != \"\":\n",
    "        GAMMA = float(new_gamma)\n",
    "    new_epsilon = input(f\"Ingrese la tasa de exploración (EPSILON) actual ({EPSILON}): \").strip()\n",
    "    if new_epsilon != \"\":\n",
    "        EPSILON = float(new_epsilon)\n",
    "    new_decay = input(f\"Ingrese el factor de decaimiento (EPSILON_DECAY) actual ({EPSILON_DECAY}): \").strip()\n",
    "    if new_decay != \"\":\n",
    "        EPSILON_DECAY = float(new_decay)\n",
    "    print(\"Nuevos parámetros:\")\n",
    "    print(f\"ALPHA = {ALPHA}, GAMMA = {GAMMA}, EPSILON = {EPSILON}, EPSILON_DECAY = {EPSILON_DECAY}\")\n",
    "\n",
    "# ---------------- Modo de Evaluación: Jugar contra el agente entrenado ---------------- #\n",
    "def play_game():\n",
    "    board = create_board()\n",
    "    game_over = False\n",
    "    turn = 0\n",
    "    print_board(board)\n",
    "    while not game_over:\n",
    "        if turn == 0:\n",
    "            valid_cols = get_valid_locations(board)\n",
    "            col = -1\n",
    "            while col not in valid_cols:\n",
    "                try:\n",
    "                    col = int(input(f\"Col (0-{COLUMN_COUNT-1}): \"))\n",
    "                    if col not in valid_cols:\n",
    "                        print(\"No válido, intente otra vez.\")\n",
    "                except ValueError:\n",
    "                    print(\"Entrada inválida.\")\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 1)\n",
    "            if winning_move(board, 1):\n",
    "                winning_pos = get_winning_positions(board, 1)\n",
    "                print_board(board, winning_pos)\n",
    "                print(\"¡Ganaste!\")\n",
    "                game_over = True\n",
    "            else:\n",
    "                print_board(board)\n",
    "        else:\n",
    "            state = get_state_one_hot(board)\n",
    "            col = choose_action(state, board, 0.0)  # Sin exploración\n",
    "            if col not in get_valid_locations(board):\n",
    "                col = random.choice(get_valid_locations(board))\n",
    "            row = get_next_open_row(board, col)\n",
    "            drop_piece(board, row, col, 2)\n",
    "            if winning_move(board, 2):\n",
    "                winning_pos = get_winning_positions(board, 2)\n",
    "                print_board(board, winning_pos)\n",
    "                print(\"IA gana.\")\n",
    "                game_over = True\n",
    "            else:\n",
    "                print_board(board)\n",
    "        turn = (turn + 1) % 2\n",
    "\n",
    "# ---------------- Opciones de Ejecución ---------------- #\n",
    "def ejecutar():\n",
    "    mode_input = input(\"Elija 'train' para entrenamiento, 'play' para jugar, o 'tune' para ajustar parámetros: \").strip().lower()\n",
    "    if mode_input == \"train\":\n",
    "        episodes = int(input(\"Ingrese el número de episodios para entrenar: \"))\n",
    "        train_agent(episodes)\n",
    "    elif mode_input == \"play\":\n",
    "        play_game()\n",
    "    elif mode_input == \"tune\":\n",
    "        fine_tuning()\n",
    "    else:\n",
    "        print(\"Opción no válida.\")\n",
    "\n",
    "# Variable global para acceso al tablero actual en choose_action (solo para este ejemplo)\n",
    "current_board = None\n",
    "\n",
    "ejecutar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Juego de suma cero  Gana o pierda todo  Presentar juegos de suma cero en la teoria del juego - FasterCapital. (s. f.). FasterCapital. https://fastercapital.com/es/contenido/Juego-de-suma-cero--Gana-o-pierda-todo--Presentar-juegos-de-suma-cero-en-la-teoria-del-juego.html\n",
    "\n",
    "- Alex. (s. f.). Teoría de juegos: Estrategia y toma de decisiones en situaciones competitivas. AproximadaMente. Pensar Mejor y Tomar Mejores Decisiones. https://www.aproximadamente.com/teoria-de-juegos%3A-estrategia-y-toma-de-decisiones-en-situaciones-competitivas/\n",
    "\n",
    "- How can you use temporal difference learning in a reinforcement learning project? (2024, 8 enero). https://www.linkedin.com/advice/0/how-can-you-use-temporal-difference-learning-99ncc\n",
    "\n",
    "- Cerretani, J. (2024, 20 diciembre). Aprendizaje por refuerzo (RL) — Capítulo 1: Historia del aprendizaje por refuerzo — Parte 3: Aprendizaje por diferencia temporal. Medium. https://medium.com/%40joancerretanids/aprendizaje-por-refuerzo-rl-cap%C3%ADtulo-1-historia-del-aprendizaje-por-refuerzo-parte-3-fc4d17197680\n",
    "\n",
    "- Libretexts. (2022, 2 noviembre). 4: Juegos que no son de suma cero. LibreTexts Español. https://espanol.libretexts.org/Matematicas/Matematicas_Aplicadas/Introduccion_a_la_teoria_de_juegos%3A_un_enfoque_de_descubrimiento_%28Nordstrom%29/04%3A_Juegos_que_no_son_de_suma_cero\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
