{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoja de Trabajo 2\n",
    "\n",
    "## Integrantes\n",
    "\n",
    "### Sergio Orellana - 221122\n",
    "\n",
    "### Andre Marroquin - 22266\n",
    "\n",
    "### Rodrigo Mansilla - 22611\n",
    "\n",
    "# Link del repositorio\n",
    "\n",
    "https://github.com/mar22266/LABORATORIOS-IA.git\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Preguntas Teóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "1. Defina el proceso de decisión de Markov (MDP) y explique sus componentes.\n",
    "\n",
    "        R//\n",
    "\n",
    "\n",
    "2. Describa cual es la diferencia entre política, evaluación de políticas, mejora de políticas e iteración de políticas en el contexto de los PDM.\n",
    "\n",
    "        R//\n",
    "\n",
    "\n",
    "\n",
    "3. Explique el concepto de factor de descuento (gamma) en los MDP. ¿Cómo influye en la toma de decisiones?\n",
    "\n",
    "        R//\n",
    "\n",
    "\n",
    "4. Analice la diferencia entre los algoritmos de iteración de valores y de iteración de políticas para resolver MDP.\n",
    "\n",
    "        R//\n",
    "\n",
    "\n",
    "5. ¿Cuáles son algunos desafíos o limitaciones comunes asociados con la resolución de MDP a gran escala?\n",
    "Discuta los enfoques potenciales para abordar estos desafíos.\n",
    "\n",
    "        R//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Preguntas Analíticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "\n",
    "1. Analice críticamente los supuestos subyacentes a la propiedad de Markov en los Procesos de Decisión de\n",
    "Markov (MDP). Analice escenarios en los que estos supuestos puedan no ser válidos y sus implicaciones\n",
    "para la toma de decisiones.\n",
    "\n",
    "        R//\n",
    "\n",
    "\n",
    "2. Explore los desafíos de modelar la incertidumbre en los procesos de decisión de Markov (MDP) y analice\n",
    "estrategias para una toma de decisiones sólida en entornos inciertos.\n",
    "\n",
    "        R//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Preguntas Prácticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entorno Frozen Lake:\n",
      "S F F F\n",
      "F F F F\n",
      "F F F F\n",
      "F F F G\n",
      "\n",
      "Funcion de valor V(s):\n",
      "[[0.95099005 0.96059601 0.970299   0.9801    ]\n",
      " [0.96059601 0.970299   0.9801     0.99      ]\n",
      " [0.970299   0.9801     0.99       1.        ]\n",
      " [0.9801     0.99       1.         0.        ]]\n",
      "\n",
      "Politica optima:\n",
      "S → → ↓\n",
      "→ → → ↓\n",
      "→ → → ↓\n",
      "→ → → G\n",
      "\n",
      "Trayecto simulado:\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(1, 3)\n",
      "(2, 3)\n",
      "(3, 3)\n",
      "\n",
      "Simulacion completada.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Establecer semilla para reproducibilidad\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Dimension del entorno 4x4\n",
    "N = 4\n",
    "\n",
    "# Definir las posibles configuraciones de inicio y meta \n",
    "configuraciones = [\n",
    "    ((0, 0), (N - 1, N - 1)),\n",
    "    ((0, N - 1), (N - 1, 0)),\n",
    "    ((N - 1, 0), (0, N - 1)),\n",
    "    ((N - 1, N - 1), (0, 0))\n",
    "]\n",
    "start, goal = random.choice(configuraciones)\n",
    "\n",
    "# Determinar aleatoriamente el numero de hoyos entre 0 y 3\n",
    "n_holes = random.randint(0, 3)\n",
    "\n",
    "# Seleccionar posiciones aleatorias para los hoyos \n",
    "posibles_posiciones = [(i, j) for i in range(N) for j in range(N) if (i, j) not in [start, goal]]\n",
    "holes = random.sample(posibles_posiciones, n_holes)\n",
    "\n",
    "# Crear el entorno\n",
    "grid = [['F' for _ in range(N)] for _ in range(N)]\n",
    "grid[start[0]][start[1]] = 'S'\n",
    "grid[goal[0]][goal[1]] = 'G'\n",
    "for h in holes:\n",
    "    grid[h[0]][h[1]] = 'H'\n",
    "\n",
    "# Mostrar el entorno generado\n",
    "print(\"Entorno Frozen Lake:\")\n",
    "for row in grid:\n",
    "    print(' '.join(row))\n",
    "\n",
    "# Definicion de la dinamica del MDP\n",
    "# 0: arriba, 1: derecha, 2: abajo, 3: izquierda\n",
    "acciones = {\n",
    "    0: (-1, 0),  \n",
    "    1: (0, 1),   \n",
    "    2: (1, 0),   \n",
    "    3: (0, -1)   \n",
    "}\n",
    "\n",
    "# Retorna True si el estado es terminal meta o hoyo\n",
    "def es_terminal(state):\n",
    "    i, j = state\n",
    "    return grid[i][j] in ['G', 'H']\n",
    "\n",
    "\n",
    "# Dado un estado y una accion, retorna el siguiente estado. Si la accion lleva fuera de limites, se queda en el mismo estado. Si el estado ya es terminal, no se realiza movimiento.\n",
    "def obtener_siguiente_estado(state, action):\n",
    "    if es_terminal(state):\n",
    "        return state\n",
    "    di, dj = acciones[action]\n",
    "    new_state = (state[0] + di, state[1] + dj)\n",
    "    # Verificar limites del entorno\n",
    "    if new_state[0] < 0 or new_state[0] >= N or new_state[1] < 0 or new_state[1] >= N:\n",
    "        return state\n",
    "    return new_state\n",
    "\n",
    "# Retorna la recompensa de la transicion. Se muestra 1 si se llega a la meta, 0 en otro caso.\n",
    "def obtener_recompensa(state, next_state):\n",
    "    i, j = next_state\n",
    "    if grid[i][j] == 'G':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Algoritmo de Iteracion de Valores para resolver el MDP\n",
    "# Parametros del MDP\n",
    "gamma = 0.99   # Factor de descuento\n",
    "theta = 1e-4   # Criterio de convergencia\n",
    "\n",
    "# Inicializar la funcion de valor V(s) para cada estado en la grilla\n",
    "V = np.zeros((N, N))\n",
    "\n",
    "# Implementacion del algoritmo de iteracion de valores.\n",
    "def value_iteration():\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Iterar sobre cada celda \n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                state = (i, j)\n",
    "                if es_terminal(state):\n",
    "                    continue\n",
    "                v = V[i, j]\n",
    "                # Calcular el valor de cada accion posible\n",
    "                valores_acciones = []\n",
    "                for a in acciones.keys():\n",
    "                    next_state = obtener_siguiente_estado(state, a)\n",
    "                    r = obtener_recompensa(state, next_state)\n",
    "                    # Como la transicion es deterministica:\n",
    "                    valores_acciones.append(r + gamma * V[next_state[0], next_state[1]])\n",
    "                # Actualizar V(s) con el maximo valor\n",
    "                V[i, j] = max(valores_acciones)\n",
    "                delta = max(delta, abs(v - V[i, j]))\n",
    "        # Verificar convergencia\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "# Extrae la politica optima a partir de la funcion de valor V. Para cada estado no terminal, se elige la accion que maximiza la recompensa futura.\n",
    "def extraer_politica():\n",
    "    policy = np.full((N, N), -1, dtype=int)  \n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            state = (i, j)\n",
    "            if es_terminal(state):\n",
    "                policy[i, j] = -1\n",
    "            else:\n",
    "                valores_acciones = {}\n",
    "                for a in acciones.keys():\n",
    "                    next_state = obtener_siguiente_estado(state, a)\n",
    "                    r = obtener_recompensa(state, next_state)\n",
    "                    valores_acciones[a] = r + gamma * V[next_state[0], next_state[1]]\n",
    "                mejor_accion = max(valores_acciones, key=valores_acciones.get)\n",
    "                policy[i, j] = mejor_accion\n",
    "    return policy\n",
    "\n",
    "# Ejecutar la iteracion de valores para calcular V(s)\n",
    "value_iteration()\n",
    "print(\"\\nFuncion de valor V(s):\")\n",
    "print(V)\n",
    "\n",
    "# Extraer la politica optima\n",
    "policy = extraer_politica()\n",
    "\n",
    "# Mapear acciones a simbolos para visualizacion\n",
    "# 0: ↑, 1: →, 2: ↓, 3: ←, -1: T estado terminal\n",
    "action_symbols = {0: '↑', 1: '→', 2: '↓', 3: '←', -1: 'T'}\n",
    "\n",
    "print(\"\\nPolitica optima:\")\n",
    "for i in range(N):\n",
    "    row_symbols = []\n",
    "    for j in range(N):\n",
    "        # Mostrar S, G o H segun corresponda; de lo contrario, el simbolo de la accion\n",
    "        if (i, j) == start:\n",
    "            row_symbols.append('S')\n",
    "        elif (i, j) == goal:\n",
    "            row_symbols.append('G')\n",
    "        elif grid[i][j] == 'H':\n",
    "            row_symbols.append('H')\n",
    "        else:\n",
    "            row_symbols.append(action_symbols[policy[i, j]])\n",
    "    print(' '.join(row_symbols))\n",
    "\n",
    "# Funcion que encapsula la ejecucion del MDP y la visualizacion de resultados.\n",
    "def ejecutar_simulacion():\n",
    "    current_state = start\n",
    "    trajectory = [current_state]\n",
    "    # Simular trayecto siguiendo la politica optima hasta llegar a un estado terminal\n",
    "    while not es_terminal(current_state):\n",
    "        action = policy[current_state[0]][current_state[1]]\n",
    "        next_state = obtener_siguiente_estado(current_state, action)\n",
    "        trajectory.append(next_state)\n",
    "        current_state = next_state\n",
    "        # Evitar bucle infinito en caso de ciclos\n",
    "        if len(trajectory) > 100:\n",
    "            break\n",
    "    print(\"\\nTrayecto simulado:\")\n",
    "    for state in trajectory:\n",
    "        print(state)\n",
    "    print(\"\\nSimulacion completada.\")\n",
    "\n",
    "# Ejecutar la simulacion\n",
    "ejecutar_simulacion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt final:\n",
    "\n",
    "Ya con este entorno Frozen Lake en Python basado en un MDP, usando una matriz 4x4 con inicio y meta en esquinas opuestas seleccionadas aleatoriamente y hasta 3 hoyos ubicados aleatoriamente necesito que me ayudes a termines el trayecto simulado a partir de la politica optima porfavor.\n",
    "\n",
    "Reflexión:\n",
    "\n",
    "Para obtener el resultado deseado, se precisaron ajustes en el prompt, especificando claramente el formato y las modificaciones requeridas en el código. Con estas precisiones se logró que el código cumpliera con los objetivos sin hacer la totalidad del trabajo. Siempre siendo muy claros para ayudar y simplificar el resultado del prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
